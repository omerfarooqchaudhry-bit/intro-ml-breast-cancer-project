{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e9f140",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 116)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:116\u001b[0;36m\u001b[0m\n\u001b[0;31m    main()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import seaborn as sns\n",
    "    \n",
    "   # STEP 1: Download the Dataset \n",
    "    dataset = pd.read_csv('data.csv')    \n",
    "    print(\"DATASET\", dataset)\n",
    "    print(\"DATASET INFO\", dataset.info())\n",
    "    \n",
    "    # STEP 2 : Reading the Dataset\n",
    "    # Step 2a: Create a boolean DataFrame showing missing values (True/False)\n",
    "    missing_values_dataframe = dataset.isna()\n",
    "    print(\"BOOLEAN MISSING VALUE MASK:\")\n",
    "    print(missing_values_dataframe.head())  # Shows True where data is missing\n",
    "    # Step 2b: Count total missing values per column\n",
    "    missing_counts = missing_values_dataframe.sum()\n",
    "    print(\"\\nCOUNT OF MISSING VALUES PER COLUMN:\")\n",
    "    print(missing_counts)\n",
    "    # Step 2c: Count total missing values in entire dataset\n",
    "    # This will show how many missing (NaN) values each column has — see that Unnamed: 32 has all NaNs.\n",
    "    total_missing =  missing_values_dataframe.values.sum()\n",
    "    print(\"\\nTOTAL MISSING VALUES IN DATASET:\", total_missing)\n",
    "    \"\"\"\n",
    "    Every real column (id, diagnosis, radius_mean, …, fractal_dimension_worst) has 0 missing values\n",
    "    The last column, Unnamed: 32, has 569 missing values (one for every row)\n",
    "    That means this column is completely empty and should be removed from your dataset.\n",
    "    Drop only the all-NaN column (Unnamed: 32):\n",
    "    \"\"\"\n",
    "    \n",
    "    # STEP 3 — Dropping Unnecessary Columns - Drop the Missing Column\n",
    "    dataset = dataset.drop(columns=[\"Unnamed: 32\"])\n",
    "    print(\"\\nAFTER DROPPING MISSING COLUMN:\")\n",
    "    print(dataset.isnull().sum())\n",
    "    print(\"DATASET\", dataset)\n",
    "    print(\"DATASET SHAPE:\", dataset.shape)\n",
    "\n",
    "    # STEP 4: Preprocessing\n",
    "    # 4a: Encoding - Encode Categorical Features\n",
    "    \"\"\"\n",
    "    Why: ML models can’t process strings — they need numerical values.\n",
    "    In the dataset, the only categorical feature is diagnosis (M or B). Because it’s a binary categorical feature and diagnosis is a label column — a label encoder is appropriate \n",
    "    Convert diagnosis to numeric (e.g., M → 1, B → 0). Unique values (B,M) in alphabetical order maps to 0 & 1\n",
    "    \"\"\"\n",
    "    dataset[\"diagnosis\"] = dataset[\"diagnosis\"].map({\"M\": 1, \"B\": 0})\n",
    "    print(\"DATASET\", dataset)\n",
    "    # 4b:Standardization is considered one method for normalization.\n",
    "    \"\"\"\n",
    "    Normalization - it is normalizing the ranges of data fields in general. Standardization is considered one method for normalization.\n",
    "    Standardization is useful when the values of the feature are normal distributed (i.e., the values follow the bell-shaped curve which also means the data is almost surrounding a common mean with most of the data condensed towards that mean and decreasing as the values move far from it).\n",
    "    Note: Normalization/scaling/standardization is one method of normalization\n",
    "    z(new value) = x(original value ) -   mean / variance \n",
    "    user scalaer from sklearn to calculate new values or use numpy to calculate mean, std dev and variane\n",
    "    We train model on training data\n",
    "    We hold back test data and test data is unseen data\n",
    "    \"\"\"\n",
    "    # Before scaling/normalization\n",
    "    print(\"\\nENTIRE DATA FRAME:\")\n",
    "    print(dataset)    \n",
    "    # Normalaization\n",
    "    scalar = StandardScaler()\n",
    "    data_refined = scalar.fit_transform(dataset)\n",
    "    print(\"\\nDATA FRAME WITH NORMALIZATION:\")\n",
    "    print(\"REFINED DATA\", data_refined)\n",
    "    print(\"TYPE\", type(data_refined))\n",
    "    # Convert back to DataFrame\n",
    "    # columns=dataset.columns → restores original column names\n",
    "    data_refined_df = pd.DataFrame(data_refined, columns=dataset.columns)\n",
    "    print(\"\\nREFINED DATA (as DataFrame):\")\n",
    "    print(data_refined_df.head())\n",
    "    # Save to CSV\n",
    "    data_refined_df.to_csv(\"data_refined.csv\", index=False)\n",
    "    data = pd.read_csv(\"data_refined.csv\")\n",
    "    print(\"\\n✅ FIRST 5 ROWS:\")\n",
    "    print(data.head())\n",
    "    print(\"\\n✅ LAST 5 ROWS:\")\n",
    "    print(data.tail())\n",
    "    # 4c: Dealing with Nan values - see steps 2c & above\n",
    "     \n",
    "    # STEP 5: Visualization\n",
    "    #  Load the preprocessed CSV\n",
    "    dataset = pd.read_csv(\"data_refined.csv\")\n",
    "    # If the 'id' column exists, drop it for visualizations\n",
    "    dataset_viz = dataset.drop(columns=[\"id\"], errors='ignore')\n",
    "    # Pair Plot - shows scatter plots between all feature pairs and histograms for each feature. It’s useful to see relationships and clusters.\n",
    "    sns.set_theme(style=\"ticks\", palette=\"pastel\")\n",
    "    # If you have the target column 'diagnosis', color by it\n",
    "    sns.pairplot(dataset_viz, hue=\"diagnosis\", diag_kind=\"hist\")\n",
    "    plt.suptitle(\"Pair Plot of Features\", y=1.02)\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    Notes:\n",
    "    hue=\"diagnosis\" colors malignant vs benign points differently\n",
    "    diag_kind=\"hist\" shows histograms along the diagonal  \n",
    "    \"\"\"\n",
    "    # Correlation Matrix Heatmap - shows feature correlations, useful to detect highly correlated features.\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    corr = dataset_viz.corr()  # correlation matrix\n",
    "    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
    "    plt.title(\"Correlation Matrix Heatmap\")\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    Notes:\n",
    "    Correlation values close to 1 or -1 indicate strong positive/negative correlation\n",
    "    \"\"\"\n",
    "    # Box Plots - help detect outliers and see feature distributions.\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    sns.boxplot(data=dataset_viz.drop(columns=[\"diagnosis\"], errors='ignore'))\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Box Plots of Features\")\n",
    "    plt.show()\n",
    "   \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
